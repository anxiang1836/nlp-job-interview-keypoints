# SoftLexicon

> **paper :** [Simplify the Usage of Lexicon in Chinese NER](https://arxiv.org/pdf/1908.05969.pdf)
>
> **source:** ACL 2020
>
> **code:** [SoftLexicon](https://github.com/v-mipeng/LexiconAugmentedNER)

提出了一个简单而有效的方法将词汇信息纳入字符表示。这种方法避免了设计复杂的序列建模结构，并且对于任何NER模型，它只需要细微地调整字符表示层来引入词典信息。

所提出的方法可转移到不同的序列标记架构，并可以很容易地与预训练模型结合，如BERT。

模型结构如下：

![](https://pictrue-bed.oss-cn-beijing.aliyuncs.com/20220912010500.png)

##  1: 融合词汇信息

纯粹基于字符的NER模型的问题是它不能利用单词信息。为了解决这个问题，本文提出了SoftLexicon方法

它不只保留每个字符在进行分词后的一个分词结果，而是使用词典获得的所有可能的分词结果。

为了保留分词信息，匹配到的词中的每个字符都被分类到 {B,M,E,S}中。对于每个字符$ci$，这四个集合被认为为:

- B：当前字为词的词首；
- M：当前字在词的中间；
- E：当前字为词的词尾；
- S：当前自独立可以成词的词：

<img src="https://pictrue-bed.oss-cn-beijing.aliyuncs.com/20220912012013.png" style="zoom:85%;" />



> PS：这种挂词方式就很好的解决了WC-LSTM的挂词中的信息确实缺陷，对于"人和药店"，会被挂到每个字的不同set中

在获得每个字符的{B,M,E,S}词集之后，每个词集被压缩成一个固定维度的向量。同时本文探索了实现这种压缩的两种实现方法。

- 直观的方式是进行平均
  $$
  v^s(S)=\frac{1}{|S|}\sum_{w\in{S}}e^w(w)
  $$
  其中$S$提供关于词汇的集合，$$e^w$$贡献了单词的embedding

  > 然而这种方法表现并不佳，同时为了保持计算效率，本文没有选择像注意力这样的动态加权算法。
  >
  > 相反，本文建议使用每个单词的频率来表示它的权重。（由于一个单词的出现频率是一个可以离线获取的静态值，这样可以大大加快每个单词权重的计算）

- 全局平均

  让$$z(w)$$表示词典中词$w$在统计数据中出现的频率，词$$S$$的加权表示如下：
  $$
  v^s(S)=\frac{4}{Z}z(w)e^w(w)
  $$
  其中，
  $$
  Z=\sum_{w\in B \cup M \cup E \cup S}z(w)
  $$
  同时如果$$w$$被另一个与词典匹配的子序列覆盖，则w的频率不会增加。（这防止了较短单词的频率总是小于覆盖它的较长单词的频率的问题）

用$$s={c_1, c_2, ..., c_n}$$表示中文句子，其中$c_i$表示第$i$个字。

最后一步，是将四个集合的embedding组合成一个固定维的特征，并将其添加到每个字符表示中：
$$
e^s(B,M,E,S)=[v^s(B);v^s(M);v^s(E);v^s(S)]
$$

$$
x^c \leftarrow [x^c;e^s(B,M,E,S)]
$$

$$v^s$$为上述表示的加权函数。

## 2: 特点总结

- 优点

法没有造成信息损失，同时又可以引入word embedding；

模型无关，可以适配于其他序列标注框架。