# Roberta

> 在XLNet全面超越Bert后没多久，Facebook提出了RoBERTa（a Robustly Optimized BERT Pretraining Approach）。再度在多个任务上达到SOTA。那么它到底改进了什么？它在模型层面没有改变Google的Bert，**改变的只是预训练的方法**。

[https://www.jianshu.com/p/eddf04ba8545](https://www.jianshu.com/p/eddf04ba8545)

[https://blog.csdn.net/ljp1919/article/details/100666563](https://blog.csdn.net/ljp1919/article/details/100666563)

brightmart的参数：[https://github.com/brightmart/roberta\_zh](https://github.com/brightmart/roberta_zh)

